{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "TRAIN_VIDEO_DIR = 'MLDS_hw2_1_data/training_data/feat/'\n",
    "TEST_VIDEO_DIR = 'MLDS_hw2_1_data/testing_data/feat/'\n",
    "TRAIN_LABEL_DIR = 'MLDS_hw2_1_data/training_label.json'\n",
    "TEST_LABEL_DIR = 'MLDS_hw2_1_data/testing_label.json'\n",
    "TRAIN_ID_DIR = 'MLDS_hw2_1_data/training_id.txt'\n",
    "TEST_ID_DIR = 'MLDS_hw2_1_data/testing_id.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted video features into X,  label into y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = pd.read_json(TRAIN_LABEL_DIR)\n",
    "test_label = pd.read_json(TEST_LABEL_DIR)\n",
    "train_id = pd.read_csv(TRAIN_ID_DIR, header=None, names=['id'])\n",
    "test_id = pd.read_csv(TEST_ID_DIR, header=None, names=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(train_id.id):\n",
    "    v_dir = TRAIN_VIDEO_DIR + v + '.npy'\n",
    "    X_train.append(np.load(v_dir))    \n",
    "    y_train.append(train_label.loc[i, ['caption']].tolist()[0])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = list(map(list, zip(*[y[:5] for y in y_train]))) # Sample 5 labels for each sample > sample size = 5*1450\n",
    "y_train = [[text_to_word_sequence(s) for s in lst] for lst in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(test_id.id):\n",
    "    v_dir = TEST_VIDEO_DIR + v + '.npy'\n",
    "    X_test.append(np.load(v_dir))\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption preprocessing (add buffer tokens to sentence and convert sentence to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUFFER_TOKENS = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get the vocaboluary \n",
    "list_of_all_words = [w for sublist in y_train for item in sublist for w in item]\n",
    "counter = collections.Counter(list_of_all_words)\n",
    "vocab = {k:v for k, v in counter.items() if v > 3} # words with frequency > 3 are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create word_to_idx, and idx_to_word\n",
    "vocab = [i for i in vocab]\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "# add in BUFFER_TOKENS\n",
    "for i in range(len(BUFFER_TOKENS)):\n",
    "    idx_to_word[int(i)] = BUFFER_TOKENS[i]\n",
    "    word_to_idx[BUFFER_TOKENS[i]] = i\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word_to_idx[vocab[i]] = i + len(BUFFER_TOKENS)\n",
    "    idx_to_word[int(i + len(BUFFER_TOKENS))] = vocab[i]\n",
    "\n",
    "word_dict = {}\n",
    "word_dict['idx_to_word'] = idx_to_word\n",
    "word_dict['word_to_idx'] = word_to_idx\n",
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"word_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert sentences into encoding/integers\n",
    "# pad all sentence to length of padding_len - 2 \n",
    "def _convert_sentence_to_numbers(s):\n",
    "    \"\"\"Convert a sentence s (a list of words) to list of numbers using word_to_idx\"\"\"\n",
    "    UNK_IDX = BUFFER_TOKENS.index('<UNK>')\n",
    "    PAD_IDX = BUFFER_TOKENS.index('<PAD>')\n",
    "    START_TOKEN = BUFFER_TOKENS.index('<BOS>')\n",
    "    END_IDX = BUFFER_TOKENS.index('<EOS>')\n",
    "    padding_len = 44\n",
    "    s_encoded = [START_TOKEN]\n",
    "    s_encoded += [word_to_idx.get(w) for w in s if w in word_to_idx]\n",
    "    s_encoded += [END_IDX]\n",
    "    s_encoded += [PAD_IDX] * (padding_len - len(s_encoded))\n",
    "    return s_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = [[_convert_sentence_to_numbers(s) for s in lst] for lst in y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nm_epochs = 10\n",
    "input_embedding_size = 128\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = 256\n",
    "train_num_batches_per_epoch = len(X_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, 80, 4096), dtype=tf.float32)\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(source, target, batch_size):\n",
    "    # Shuffle data\n",
    "    source = np.array(source)\n",
    "    target = np.array(target)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "    source = source[shuffle_indices]\n",
    "    target = target[shuffle_indices]\n",
    "    \n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        seqlen_batch = [list(row).index(2) for row in target_batch]\n",
    "\n",
    "        yield np.array(source_batch), np.array(target_batch), np.array(seqlen_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_inputs, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "        decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "            decoder_cell, decoder_inputs_embedded,\n",
    "            initial_state=encoder_final_state, \n",
    "            sequence_length=target_seq_len, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "    \n",
    "    return encoder_final_state, decoder_final_state, decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    final_preds = []\n",
    "    encoder_final_state, decoder_final_state, decoder_logits = build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units)\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "        logits=decoder_logits,\n",
    "    )\n",
    "\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for train_y in Y_train:\n",
    "            for epoch in range(nm_epochs):\n",
    "                training_loss = 0.0\n",
    "                for epoch_x, epoch_y, epoch_seqlen in next_batch(X_train, train_y, batch_size):\n",
    "                    _, c = sess.run([optimizer, loss], feed_dict={encoder_inputs: epoch_x, decoder_inputs: epoch_y[:, :-1], \n",
    "                                                                  decoder_targets: epoch_y[:, 1:], target_seq_len: epoch_seqlen})\n",
    "                    training_loss += c / train_num_batches_per_epoch\n",
    "                print('Epoch {} training loss: {}'.format(str(epoch+1)+'/'+str(nm_epochs), training_loss))\n",
    "        \n",
    "        saver.save(sess, \"models/lstm_model_10.ckpt\")\n",
    "        \n",
    "        for x_test in X_test:\n",
    "            preds = []\n",
    "            current_pred = np.ones([1,1])\n",
    "            x_test = np.expand_dims(x_test, axis=0)\n",
    "            state = sess.run(encoder_final_state, feed_dict={encoder_inputs: x_test})\n",
    "\n",
    "            for t in range(44):\n",
    "                feed_dict={decoder_inputs: current_pred, \n",
    "                           encoder_final_state: state, target_seq_len: [1]}\n",
    "                current_pred, state = sess.run([decoder_prediction, decoder_final_state], feed_dict=feed_dict)\n",
    "                if current_pred == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    preds.append(current_pred[0][0])\n",
    "                    current_pred = current_pred.reshape(-1, 1)\n",
    "            final_preds.append(preds)\n",
    "        \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [[word_dict['idx_to_word'][_id] for _id in row] for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output_10.txt', 'w') as f:\n",
    "    for i, t in zip(test_id.id, text):\n",
    "        f.write('{},{}\\n'.format(i, ' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

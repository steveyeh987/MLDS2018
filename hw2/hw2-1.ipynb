{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "TRAIN_VIDEO_DIR = 'MLDS_hw2_1_data/training_data/feat/'\n",
    "TEST_VIDEO_DIR = 'MLDS_hw2_1_data/testing_data/feat/'\n",
    "TRAIN_LABEL_DIR = 'MLDS_hw2_1_data/training_label.json'\n",
    "TEST_LABEL_DIR = 'MLDS_hw2_1_data/testing_label.json'\n",
    "TRAIN_ID_DIR = 'MLDS_hw2_1_data/training_id.txt'\n",
    "TEST_ID_DIR = 'MLDS_hw2_1_data/testing_id.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted video features into X,  label into y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_json(TRAIN_LABEL_DIR)\n",
    "test_label = pd.read_json(TEST_LABEL_DIR)\n",
    "train_id = pd.read_csv(TRAIN_ID_DIR, header=None, names=['id'])\n",
    "test_id = pd.read_csv(TEST_ID_DIR, header=None, names=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(train_id.id):\n",
    "    v_dir = TRAIN_VIDEO_DIR + v + '.npy'\n",
    "    X_train.append(np.load(v_dir))    \n",
    "    y_train.append(train_label.loc[i, ['caption']].tolist()[0])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = list(map(list, zip(*[y[:5] for y in y_train]))) # Sample 5 labels for each sample > sample size = 5*1450\n",
    "y_train = [[text_to_word_sequence(s) for s in lst] for lst in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(test_id.id):\n",
    "    v_dir = TEST_VIDEO_DIR + v + '.npy'\n",
    "    X_test.append(np.load(v_dir))\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption preprocessing (add buffer tokens to sentence and convert sentence to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_TOKENS = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the vocaboluary \n",
    "list_of_all_words = [w for sublist in y_train for item in sublist for w in item]\n",
    "counter = collections.Counter(list_of_all_words)\n",
    "vocab = {k:v for k, v in counter.items() if v > 3} # words with frequency > 3 are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word_to_idx, and idx_to_word\n",
    "vocab = [i for i in vocab]\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "# add in BUFFER_TOKENS\n",
    "for i in range(len(BUFFER_TOKENS)):\n",
    "    idx_to_word[int(i)] = BUFFER_TOKENS[i]\n",
    "    word_to_idx[BUFFER_TOKENS[i]] = i\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word_to_idx[vocab[i]] = i + len(BUFFER_TOKENS)\n",
    "    idx_to_word[int(i + len(BUFFER_TOKENS))] = vocab[i]\n",
    "\n",
    "word_dict = {}\n",
    "word_dict['idx_to_word'] = idx_to_word\n",
    "word_dict['word_to_idx'] = word_to_idx\n",
    "vocab_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences into encoding/integers\n",
    "# pad all sentence to length of padding_len - 2 \n",
    "def _convert_sentence_to_numbers(s):\n",
    "    \"\"\"Convert a sentence s (a list of words) to list of numbers using word_to_idx\"\"\"\n",
    "    UNK_IDX = BUFFER_TOKENS.index('<UNK>')\n",
    "    PAD_IDX = BUFFER_TOKENS.index('<PAD>')\n",
    "    START_TOKEN = BUFFER_TOKENS.index('<BOS>')\n",
    "    END_IDX = BUFFER_TOKENS.index('<EOS>')\n",
    "    padding_len = 44\n",
    "    s_encoded = [START_TOKEN]\n",
    "    s_encoded += [word_to_idx.get(w) for w in s if w in word_to_idx]\n",
    "    s_encoded += [END_IDX]\n",
    "    s_encoded += [PAD_IDX] * (padding_len - len(s_encoded))\n",
    "    return s_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [[_convert_sentence_to_numbers(s) for s in lst] for lst in y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nm_epochs = 10\n",
    "input_embedding_size = 128\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = 256\n",
    "train_num_batches_per_epoch = len(X_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, 80, 4096), dtype=tf.float32)\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(source, target, batch_size):\n",
    "    # Shuffle data\n",
    "    source = np.array(source)\n",
    "    target = np.array(target)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "    source = source[shuffle_indices]\n",
    "    target = target[shuffle_indices]\n",
    "    \n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        seqlen_batch = [list(row).index(2) for row in target_batch]\n",
    "\n",
    "        yield np.array(source_batch), np.array(target_batch), np.array(seqlen_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_inputs, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "        decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "            decoder_cell, decoder_inputs_embedded,\n",
    "            initial_state=encoder_final_state, \n",
    "            sequence_length=target_seq_len, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "    \n",
    "    return encoder_final_state, decoder_final_state, decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    final_preds = []\n",
    "    encoder_final_state, decoder_final_state, decoder_logits = build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units)\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "        logits=decoder_logits,\n",
    "    )\n",
    "\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for train_y in Y_train:\n",
    "            for epoch in range(nm_epochs):\n",
    "                training_loss = 0.0\n",
    "                for epoch_x, epoch_y, epoch_seqlen in next_batch(X_train, train_y, batch_size):\n",
    "                    _, c = sess.run([optimizer, loss], feed_dict={encoder_inputs: epoch_x, decoder_inputs: epoch_y[:, :-1], \n",
    "                                                                  decoder_targets: epoch_y[:, 1:], target_seq_len: epoch_seqlen})\n",
    "                    training_loss += c / train_num_batches_per_epoch\n",
    "                print('Epoch {} training loss: {}'.format(str(epoch+1)+'/'+str(nm_epochs), training_loss))\n",
    "        \n",
    "        saver.save(sess, \"models/lstm_model_10.ckpt\")\n",
    "        \n",
    "        for x_test in X_test:\n",
    "            preds = []\n",
    "            current_pred = np.ones([1,1])\n",
    "            x_test = np.expand_dims(x_test, axis=0)\n",
    "            state = sess.run(encoder_final_state, feed_dict={encoder_inputs: x_test})\n",
    "\n",
    "            for t in range(44):\n",
    "                feed_dict={decoder_inputs: current_pred, \n",
    "                           encoder_final_state: state, target_seq_len: [1]}\n",
    "                current_pred, state = sess.run([decoder_prediction, decoder_final_state], feed_dict=feed_dict)\n",
    "                if current_pred == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    preds.append(current_pred[0][0])\n",
    "                    current_pred = current_pred.reshape(-1, 1)\n",
    "            final_preds.append(preds)\n",
    "        \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-e5671941121d>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch 1/10 training loss: 6.717007291728053\n",
      "Epoch 2/10 training loss: 6.492694361456509\n",
      "Epoch 3/10 training loss: 6.386293164614973\n",
      "Epoch 4/10 training loss: 6.289036800121438\n",
      "Epoch 5/10 training loss: 6.199580209008578\n",
      "Epoch 6/10 training loss: 6.115524752386687\n",
      "Epoch 7/10 training loss: 6.0368329081042065\n",
      "Epoch 8/10 training loss: 5.9619857360576765\n",
      "Epoch 9/10 training loss: 5.889075558761069\n",
      "Epoch 10/10 training loss: 5.8183196330892635\n",
      "Epoch 1/10 training loss: 5.948575611772209\n",
      "Epoch 2/10 training loss: 5.856816341137065\n",
      "Epoch 3/10 training loss: 5.780061820457721\n",
      "Epoch 4/10 training loss: 5.710085951048752\n",
      "Epoch 5/10 training loss: 5.642191278523413\n",
      "Epoch 6/10 training loss: 5.575522669430437\n",
      "Epoch 7/10 training loss: 5.51033117031229\n",
      "Epoch 8/10 training loss: 5.446028002377214\n",
      "Epoch 9/10 training loss: 5.382194288845719\n",
      "Epoch 10/10 training loss: 5.318393740160711\n",
      "Epoch 1/10 training loss: 5.353166037592393\n",
      "Epoch 2/10 training loss: 5.273360531905601\n",
      "Epoch 3/10 training loss: 5.199598098623342\n",
      "Epoch 4/10 training loss: 5.13150797219112\n",
      "Epoch 5/10 training loss: 5.06478919654057\n",
      "Epoch 6/10 training loss: 5.000601817821634\n",
      "Epoch 7/10 training loss: 4.935248983317409\n",
      "Epoch 8/10 training loss: 4.871635930291538\n",
      "Epoch 9/10 training loss: 4.81066712017717\n",
      "Epoch 10/10 training loss: 4.748708724975586\n",
      "Epoch 1/10 training loss: 4.842772714022932\n",
      "Epoch 2/10 training loss: 4.757360113078149\n",
      "Epoch 3/10 training loss: 4.679382274890767\n",
      "Epoch 4/10 training loss: 4.607300593935212\n",
      "Epoch 5/10 training loss: 4.54009171189933\n",
      "Epoch 6/10 training loss: 4.473815802870127\n",
      "Epoch 7/10 training loss: 4.407501730425605\n",
      "Epoch 8/10 training loss: 4.345226550924367\n",
      "Epoch 9/10 training loss: 4.281049827049519\n",
      "Epoch 10/10 training loss: 4.220796913936221\n",
      "Epoch 1/10 training loss: 4.354952532669594\n",
      "Epoch 2/10 training loss: 4.27037995437096\n",
      "Epoch 3/10 training loss: 4.1957519465479365\n",
      "Epoch 4/10 training loss: 4.123680295615361\n",
      "Epoch 5/10 training loss: 4.055520279654141\n",
      "Epoch 6/10 training loss: 3.9902187133657514\n",
      "Epoch 7/10 training loss: 3.92700350695643\n",
      "Epoch 8/10 training loss: 3.8648828884650923\n",
      "Epoch 9/10 training loss: 3.8051361297738966\n",
      "Epoch 10/10 training loss: 3.74484816090814\n"
     ]
    }
   ],
   "source": [
    "predictions = train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[448, 660, 910, 75, 448, 828, 1096, 992],\n",
       " [448, 910, 102, 783, 448],\n",
       " [448, 125, 910, 578, 448, 958],\n",
       " [448, 668, 910, 578, 275, 448, 990],\n",
       " [448, 660, 910, 547, 812, 98, 246],\n",
       " [448, 668, 910, 160, 448, 219],\n",
       " [448, 990, 910, 578, 448, 1080],\n",
       " [448, 959, 910, 1231, 812, 448],\n",
       " [448, 660, 910, 1301, 448, 499],\n",
       " [448, 660, 910, 434, 448, 1112, 1096, 299],\n",
       " [448, 401, 910, 1109, 448, 1112, 1096, 299],\n",
       " [448, 125, 910, 1301, 812, 448, 499],\n",
       " [448, 660, 910, 448, 334],\n",
       " [448, 487, 910, 448, 1112, 1096, 992],\n",
       " [448, 660, 910, 826, 448, 1112, 1096, 1010],\n",
       " [448, 551, 1096, 306, 909, 1301, 448, 655],\n",
       " [448, 660, 910, 1301, 448, 812, 448],\n",
       " [448, 660, 910, 578, 448, 958],\n",
       " [448, 487, 910, 175, 783, 1120],\n",
       " [448, 401, 910, 1108, 448],\n",
       " [448, 125, 910, 1301, 812, 448, 1198],\n",
       " [448, 660, 910, 1274, 448, 660],\n",
       " [448, 660, 910, 978, 237, 724, 1096, 448, 587],\n",
       " [448, 1135, 910, 1109, 448, 828, 1096, 969, 275, 448, 234],\n",
       " [448, 487, 910, 34],\n",
       " [448, 660, 910, 175, 594, 448, 557],\n",
       " [448, 1135, 910, 150, 783, 448],\n",
       " [448, 990, 910, 578, 275, 448, 990],\n",
       " [448, 660, 910, 160, 448],\n",
       " [448, 660, 233, 448, 401, 909, 547, 812, 448],\n",
       " [448, 401, 910, 373, 448, 856],\n",
       " [448, 401, 910, 434, 448, 1112, 1096, 299],\n",
       " [448, 401, 910, 102, 594, 448],\n",
       " [448, 660, 910, 578, 448, 958],\n",
       " [448, 401, 910, 434, 448, 1215, 526, 448],\n",
       " [448, 660, 910, 578, 448, 958],\n",
       " [448, 660, 910, 207],\n",
       " [448, 660, 910, 274, 757],\n",
       " [448, 910, 578, 783, 1120, 453],\n",
       " [448, 401, 910, 1109, 448, 1283],\n",
       " [448, 401, 910, 1301, 448, 655],\n",
       " [448, 660, 910, 541, 999, 900, 448, 1129, 1096, 563],\n",
       " [448, 664, 804],\n",
       " [448, 668, 910, 1301, 448, 217, 812, 1120, 1085],\n",
       " [448, 660, 910, 373, 1070, 665],\n",
       " [448, 401, 910, 1109, 448, 1112, 1096, 992],\n",
       " [448, 1131, 910, 1108, 783, 448, 1002],\n",
       " [448, 660, 910, 1301, 448, 783, 448, 664],\n",
       " [448, 401, 910, 373, 448, 856],\n",
       " [448, 125, 910, 578, 275, 448, 219],\n",
       " [448, 660, 910, 434, 448, 1010],\n",
       " [448, 660, 910, 175, 814, 1120, 460],\n",
       " [448, 551, 1096, 306, 909, 207],\n",
       " [448, 660, 910, 761, 448, 727],\n",
       " [448, 401, 910, 207, 812, 1120, 1085],\n",
       " [448, 1135, 910, 541, 448, 1112, 1096, 992, 526, 448, 1129],\n",
       " [448, 401, 910, 207],\n",
       " [448, 125, 910, 578, 448, 1037],\n",
       " [448, 401, 910, 826, 448, 792],\n",
       " [448, 660, 910, 1297, 448, 727, 783, 448, 1129],\n",
       " [448, 660, 910, 1301, 448, 217],\n",
       " [448, 1135, 910, 434, 448, 1215, 526, 448, 1129],\n",
       " [448, 125, 910, 1301, 448, 729],\n",
       " [448, 660, 910, 448],\n",
       " [448, 660, 910, 175, 594, 448],\n",
       " [448, 660, 910, 434, 448, 743],\n",
       " [448, 1135, 910, 1125, 969],\n",
       " [448, 660, 910, 1301, 448, 217],\n",
       " [448, 401, 910, 1297, 1283, 783, 448, 148],\n",
       " [448, 660, 910, 91, 526, 448, 1043],\n",
       " [448, 660, 910, 448, 664],\n",
       " [448, 660, 910, 826, 1070, 665],\n",
       " [448, 660, 910, 448, 52],\n",
       " [448, 487, 910, 207],\n",
       " [448, 660, 910, 434, 448, 1112, 1096, 1010],\n",
       " [448, 660, 910, 1297, 448, 401],\n",
       " [448, 1135, 910, 434, 448, 1112, 1096, 1010],\n",
       " [448, 1135, 910, 448, 401],\n",
       " [448, 401, 910, 434, 448, 1112, 1096, 299],\n",
       " [448, 660, 910, 160, 448],\n",
       " [448, 551, 1096, 306, 909, 207],\n",
       " [448, 668, 910, 102, 783, 1120, 992],\n",
       " [448, 401, 910, 826, 1070, 665],\n",
       " [448, 45, 910, 373, 448, 1215, 526, 633],\n",
       " [448, 401, 910, 175, 783, 1120, 460],\n",
       " [448, 660, 910, 578, 448, 958],\n",
       " [448, 660, 910, 247, 812, 448, 828, 891],\n",
       " [448, 527, 910, 578, 448, 527],\n",
       " [448, 660, 910, 448, 990],\n",
       " [448, 401, 910, 373, 448, 1112, 1096, 1194],\n",
       " [448, 660, 910, 578, 448, 958],\n",
       " [448, 444, 910, 305, 448, 664],\n",
       " [448, 660, 910, 274, 757],\n",
       " [448, 1135, 910, 541, 992, 900, 992],\n",
       " [448, 1131, 910, 578, 448, 1037],\n",
       " [448, 1135, 910, 448, 52],\n",
       " [448, 551, 1096, 677, 909, 175, 812, 1120, 1085],\n",
       " [448, 660, 910, 1301, 448, 729],\n",
       " [448, 910, 578, 275, 448, 660],\n",
       " [448, 401, 910, 541, 563, 900, 448, 413]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[word_dict['idx_to_word'][_id] for _id in row] for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'man', 'is', 'drinking', 'a', 'glass', 'of', 'water'],\n",
       " ['a', 'is', 'walking', 'in', 'a'],\n",
       " ['a', 'boy', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'dog', 'is', 'playing', 'with', 'a', 'baby'],\n",
       " ['a', 'man', 'is', 'talking', 'on', 'his', 'face'],\n",
       " ['a', 'dog', 'is', 'eating', 'a', 'ball'],\n",
       " ['a', 'baby', 'is', 'playing', 'a', 'flute'],\n",
       " ['a', 'panda', 'is', 'climbing', 'on', 'a'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'bicycle'],\n",
       " ['a', 'man', 'is', 'cutting', 'a', 'piece', 'of', 'meat'],\n",
       " ['a', 'woman', 'is', 'peeling', 'a', 'piece', 'of', 'meat'],\n",
       " ['a', 'boy', 'is', 'riding', 'on', 'a', 'bicycle'],\n",
       " ['a', 'man', 'is', 'a', 'gun'],\n",
       " ['a', 'girl', 'is', 'a', 'piece', 'of', 'water'],\n",
       " ['a', 'man', 'is', 'chopping', 'a', 'piece', 'of', 'paper'],\n",
       " ['a', 'group', 'of', 'people', 'are', 'riding', 'a', 'motorcycle'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'on', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'girl', 'is', 'running', 'in', 'the'],\n",
       " ['a', 'woman', 'is', 'jumping', 'a'],\n",
       " ['a', 'boy', 'is', 'riding', 'on', 'a', 'boat'],\n",
       " ['a', 'man', 'is', 'holding', 'a', 'man'],\n",
       " ['a', 'man', 'is', 'draining', 'pasta', 'out', 'of', 'a', 'plate'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'peeling',\n",
       "  'a',\n",
       "  'glass',\n",
       "  'of',\n",
       "  'rice',\n",
       "  'with',\n",
       "  'a',\n",
       "  'knife'],\n",
       " ['a', 'girl', 'is', 'being'],\n",
       " ['a', 'man', 'is', 'running', 'down', 'a', 'road'],\n",
       " ['a', 'person', 'is', 'swimming', 'in', 'a'],\n",
       " ['a', 'baby', 'is', 'playing', 'with', 'a', 'baby'],\n",
       " ['a', 'man', 'is', 'eating', 'a'],\n",
       " ['a', 'man', 'and', 'a', 'woman', 'are', 'talking', 'on', 'a'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a', 'potato'],\n",
       " ['a', 'woman', 'is', 'cutting', 'a', 'piece', 'of', 'meat'],\n",
       " ['a', 'woman', 'is', 'walking', 'down', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'woman', 'is', 'cutting', 'a', 'tomato', 'into', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'man', 'is', 'dancing'],\n",
       " ['a', 'man', 'is', 'doing', 'exercise'],\n",
       " ['a', 'is', 'playing', 'in', 'the', 'air'],\n",
       " ['a', 'woman', 'is', 'peeling', 'a', 'shrimp'],\n",
       " ['a', 'woman', 'is', 'riding', 'a', 'motorcycle'],\n",
       " ['a', 'man', 'is', 'adding', 'sugar', 'to', 'a', 'bowl', 'of', 'ingredients'],\n",
       " ['a', 'car', 'crashes'],\n",
       " ['a', 'dog', 'is', 'riding', 'a', 'horse', 'on', 'the', 'beach'],\n",
       " ['a', 'man', 'is', 'slicing', 'an', 'onion'],\n",
       " ['a', 'woman', 'is', 'peeling', 'a', 'piece', 'of', 'water'],\n",
       " ['a', 'cat', 'is', 'jumping', 'in', 'a', 'mirror'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'in', 'a', 'car'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a', 'potato'],\n",
       " ['a', 'boy', 'is', 'playing', 'with', 'a', 'ball'],\n",
       " ['a', 'man', 'is', 'cutting', 'a', 'paper'],\n",
       " ['a', 'man', 'is', 'running', 'through', 'the', 'woods'],\n",
       " ['a', 'group', 'of', 'people', 'are', 'dancing'],\n",
       " ['a', 'man', 'is', 'mixing', 'a', 'food'],\n",
       " ['a', 'woman', 'is', 'dancing', 'on', 'the', 'beach'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'adding',\n",
       "  'a',\n",
       "  'piece',\n",
       "  'of',\n",
       "  'water',\n",
       "  'into',\n",
       "  'a',\n",
       "  'bowl'],\n",
       " ['a', 'woman', 'is', 'dancing'],\n",
       " ['a', 'boy', 'is', 'playing', 'a', 'piano'],\n",
       " ['a', 'woman', 'is', 'chopping', 'a', 'cucumber'],\n",
       " ['a', 'man', 'is', 'putting', 'a', 'food', 'in', 'a', 'bowl'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'horse'],\n",
       " ['a', 'person', 'is', 'cutting', 'a', 'tomato', 'into', 'a', 'bowl'],\n",
       " ['a', 'boy', 'is', 'riding', 'a', 'bike'],\n",
       " ['a', 'man', 'is', 'a'],\n",
       " ['a', 'man', 'is', 'running', 'down', 'a'],\n",
       " ['a', 'man', 'is', 'cutting', 'a', 'cup'],\n",
       " ['a', 'person', 'is', 'stirring', 'rice'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'horse'],\n",
       " ['a', 'woman', 'is', 'putting', 'shrimp', 'in', 'a', 'pan'],\n",
       " ['a', 'man', 'is', 'singing', 'into', 'a', 'microphone'],\n",
       " ['a', 'man', 'is', 'a', 'car'],\n",
       " ['a', 'man', 'is', 'chopping', 'an', 'onion'],\n",
       " ['a', 'man', 'is', 'a', 'fish'],\n",
       " ['a', 'girl', 'is', 'dancing'],\n",
       " ['a', 'man', 'is', 'cutting', 'a', 'piece', 'of', 'paper'],\n",
       " ['a', 'man', 'is', 'putting', 'a', 'woman'],\n",
       " ['a', 'person', 'is', 'cutting', 'a', 'piece', 'of', 'paper'],\n",
       " ['a', 'person', 'is', 'a', 'woman'],\n",
       " ['a', 'woman', 'is', 'cutting', 'a', 'piece', 'of', 'meat'],\n",
       " ['a', 'man', 'is', 'eating', 'a'],\n",
       " ['a', 'group', 'of', 'people', 'are', 'dancing'],\n",
       " ['a', 'dog', 'is', 'walking', 'in', 'the', 'water'],\n",
       " ['a', 'woman', 'is', 'chopping', 'an', 'onion'],\n",
       " ['a', 'chef', 'is', 'slicing', 'a', 'tomato', 'into', 'pieces'],\n",
       " ['a', 'woman', 'is', 'running', 'in', 'the', 'woods'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'man', 'is', 'sitting', 'on', 'a', 'glass', 'enclosure'],\n",
       " ['a', 'puppy', 'is', 'playing', 'a', 'puppy'],\n",
       " ['a', 'man', 'is', 'a', 'baby'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a', 'piece', 'of', 'bread'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'train', 'is', 'moving', 'a', 'car'],\n",
       " ['a', 'man', 'is', 'doing', 'exercise'],\n",
       " ['a', 'person', 'is', 'adding', 'water', 'to', 'water'],\n",
       " ['a', 'cat', 'is', 'playing', 'a', 'piano'],\n",
       " ['a', 'person', 'is', 'a', 'fish'],\n",
       " ['a', 'group', 'of', 'men', 'are', 'running', 'on', 'the', 'beach'],\n",
       " ['a', 'man', 'is', 'riding', 'a', 'bike'],\n",
       " ['a', 'is', 'playing', 'with', 'a', 'man'],\n",
       " ['a', 'woman', 'is', 'adding', 'ingredients', 'to', 'a', 'pot']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_10.txt', 'w') as f:\n",
    "    for i, t in zip(test_id.id, text):\n",
    "        f.write('{},{}\\n'.format(i, ' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man plays a violin on the roof of a house.',\n",
       " 'A man is standing on a roof top playing a violin.',\n",
       " 'A man is playing a violin on the roof.',\n",
       " 'A man is on a roof dancing.',\n",
       " 'A man is playing the violin on a roof.',\n",
       " 'A man is playing a fiddle on a roof.',\n",
       " 'A man is playing guitar standing on roof top.',\n",
       " 'A man is playing the violin on the top of a roof.',\n",
       " 'A young man is standing on a roof, playing a fiddle.',\n",
       " 'A man is playing a fiddle on a roof.',\n",
       " 'A guy plays a fiddle while standing on a roof.',\n",
       " 'A man is fiddling on a rooftop.',\n",
       " 'A man is fiddling on a rooftop.',\n",
       " 'A man is playing a violin on a roof.',\n",
       " 'A man is playing the violin on top of a house.',\n",
       " 'The man played the violin on the rooftop.',\n",
       " 'The man is on the roof playing a violin.',\n",
       " 'A man plays the violin on a roof.',\n",
       " 'The fiddler played the fiddle on the roof.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label['caption'][98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "TEST_VIDEO_DIR = 'MLDS_hw2_1_data/testing_data/feat/'\n",
    "TEST_ID_DIR = 'MLDS_hw2_1_data/testing_id.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = pd.read_csv(TEST_ID_DIR, header=None, names=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted video features into X,  label into y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(test_id.id):\n",
    "    v_dir = TEST_VIDEO_DIR + v + '.npy'\n",
    "    X_test.append(np.load(v_dir))\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption preprocessing (add buffer tokens to sentence and convert sentence to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create word_to_idx, and idx_to_word\n",
    "with open(\"word_dict_schedule.pkl\",\"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "vocab_size = len(word_dict['word_to_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nm_epochs = 10\n",
    "input_embedding_size = 128\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, 80, 4096), dtype=tf.float32)\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "sampling_prob = tf.placeholder(shape=(), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(source, target, batch_size):\n",
    "    # Shuffle data\n",
    "    source = np.array(source)\n",
    "    target = np.array(target)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "    source = source[shuffle_indices]\n",
    "    target = target[shuffle_indices]\n",
    "    \n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        seqlen_batch = [list(row).index(2) for row in target_batch]\n",
    "\n",
    "        yield np.array(source_batch), np.array(target_batch), np.array(seqlen_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_inputs, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "        helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "            decoder_inputs_embedded,\n",
    "            target_seq_len,\n",
    "            embedding=embeddings,\n",
    "            sampling_probability=sampling_prob)\n",
    "        \n",
    "        output_layer = Dense(vocab_size)\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell,\n",
    "            helper,\n",
    "            encoder_final_state,\n",
    "            output_layer=output_layer)\n",
    "        \n",
    "        maximum_iterations = tf.reduce_max(target_seq_len)\n",
    "        \n",
    "        decoder_outputs, decoder_final_state, seq_len = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                        decoder, output_time_major=False,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=maximum_iterations)\n",
    "\n",
    "    decoder_logits = tf.identity(decoder_outputs.rnn_output)\n",
    "    \n",
    "    return encoder_final_state, decoder_final_state, decoder_logits, maximum_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    final_preds = []\n",
    "    encoder_final_state, decoder_final_state, decoder_logits, maximum_iterations = build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units)\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    \n",
    "    targets = tf.slice(decoder_targets, [0, 0], [-1, maximum_iterations])\n",
    "    masks = tf.sequence_mask(target_seq_len, maximum_iterations, dtype=tf.float32)\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=decoder_logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=config) as sess:        \n",
    "        saver.restore(sess, \"models/lstm_model_schedule.ckpt\")\n",
    "        \n",
    "        for x_test in X_test:\n",
    "            preds = []\n",
    "            current_pred = np.ones([1,1])\n",
    "            x_test = np.expand_dims(x_test, axis=0)\n",
    "            state = sess.run(encoder_final_state, feed_dict={encoder_inputs: x_test})\n",
    "\n",
    "            for t in range(44):\n",
    "                feed_dict={decoder_inputs: current_pred, encoder_final_state: state,\n",
    "                           sampling_prob: 0.0, target_seq_len: [1]}\n",
    "                current_pred, state = sess.run([decoder_prediction, decoder_final_state], feed_dict=feed_dict)\n",
    "                if current_pred == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    preds.append(current_pred[0][0])\n",
    "                    current_pred = current_pred.reshape(-1, 1)\n",
    "            final_preds.append(preds)\n",
    "        \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dadayeh/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/lstm_model_schedule.ckpt\n"
     ]
    }
   ],
   "source": [
    "predictions = train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [[word_dict['idx_to_word'][_id] for _id in row] for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output_schedule_inference.txt', 'w') as f:\n",
    "    for i, t in zip(test_id.id, text):\n",
    "        f.write('{},{}\\n'.format(i, ' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

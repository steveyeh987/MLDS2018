{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "TEST_VIDEO_DIR = 'MLDS_hw2_1_data/testing_data/feat/'\n",
    "TEST_ID_DIR = 'MLDS_hw2_1_data/testing_id.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = pd.read_csv(TEST_ID_DIR, header=None, names=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted video features into X,  label into y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(test_id.id):\n",
    "    v_dir = TEST_VIDEO_DIR + v + '.npy'\n",
    "    X_test.append(np.load(v_dir))\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption preprocessing (add buffer tokens to sentence and convert sentence to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create word_to_idx, and idx_to_word\n",
    "with open(\"word_dict_schedule.pkl\",\"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "vocab_size = len(word_dict['word_to_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nm_epochs = 10\n",
    "input_embedding_size = 128\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, 80, 4096), dtype=tf.float32)\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "sampling_prob = tf.placeholder(shape=(), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(source, target, batch_size):\n",
    "    # Shuffle data\n",
    "    source = np.array(source)\n",
    "    target = np.array(target)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "    source = source[shuffle_indices]\n",
    "    target = target[shuffle_indices]\n",
    "    \n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        seqlen_batch = [list(row).index(2) for row in target_batch]\n",
    "\n",
    "        yield np.array(source_batch), np.array(target_batch), np.array(seqlen_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_inputs, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "        helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "            decoder_inputs_embedded,\n",
    "            target_seq_len,\n",
    "            embedding=embeddings,\n",
    "            sampling_probability=sampling_prob)\n",
    "        \n",
    "        output_layer = Dense(vocab_size)\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell,\n",
    "            helper,\n",
    "            encoder_final_state,\n",
    "            output_layer=output_layer)\n",
    "        \n",
    "        maximum_iterations = tf.reduce_max(target_seq_len)\n",
    "        \n",
    "        decoder_outputs, decoder_final_state, seq_len = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                        decoder, output_time_major=False,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=maximum_iterations)\n",
    "\n",
    "    decoder_logits = tf.identity(decoder_outputs.rnn_output)\n",
    "    \n",
    "    return encoder_final_state, decoder_final_state, decoder_logits, maximum_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    final_preds = []\n",
    "    encoder_final_state, decoder_final_state, decoder_logits, maximum_iterations = build_model(batch_size, input_embedding_size, encoder_hidden_units, decoder_hidden_units)\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    \n",
    "    targets = tf.slice(decoder_targets, [0, 0], [-1, maximum_iterations])\n",
    "    masks = tf.sequence_mask(target_seq_len, maximum_iterations, dtype=tf.float32)\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=decoder_logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=config) as sess:        \n",
    "        saver.restore(sess, \"models/lstm_model_schedule.ckpt\")\n",
    "        \n",
    "        for x_test in X_test:\n",
    "            preds = []\n",
    "            current_pred = np.ones([1,1])\n",
    "            x_test = np.expand_dims(x_test, axis=0)\n",
    "            state = sess.run(encoder_final_state, feed_dict={encoder_inputs: x_test})\n",
    "\n",
    "            for t in range(44):\n",
    "                feed_dict={decoder_inputs: current_pred, encoder_final_state: state,\n",
    "                           sampling_prob: 0.0, target_seq_len: [1]}\n",
    "                current_pred, state = sess.run([decoder_prediction, decoder_final_state], feed_dict=feed_dict)\n",
    "                if current_pred == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    preds.append(current_pred[0][0])\n",
    "                    current_pred = current_pred.reshape(-1, 1)\n",
    "            final_preds.append(preds)\n",
    "        \n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dadayeh/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/lstm_model_schedule.ckpt\n"
     ]
    }
   ],
   "source": [
    "predictions = train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[229, 489, 728, 229, 229, 692, 958, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 926, 229],\n",
       " [229, 915, 728, 926, 1140],\n",
       " [229, 489, 1240, 229, 553, 229],\n",
       " [229, 70, 728, 757, 229],\n",
       " [229, 915, 728, 926, 229],\n",
       " [229, 913, 728, 926, 1140],\n",
       " [229, 489, 728, 677, 229],\n",
       " [229, 131, 728, 171, 229, 1284, 229],\n",
       " [229, 131, 728, 229, 229],\n",
       " [229, 1038, 728, 677, 229, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 926, 229],\n",
       " [229, 549, 728, 811, 229],\n",
       " [229, 646, 958, 466, 43, 43],\n",
       " [229, 489, 728, 677, 229],\n",
       " [229, 70, 728, 926, 229, 229],\n",
       " [229, 131, 728, 229, 229],\n",
       " [229, 489, 728, 229, 229, 229, 229],\n",
       " [229, 646, 958, 466, 43],\n",
       " [229, 489, 728, 229],\n",
       " [229, 489, 728, 40, 229, 229, 229, 556],\n",
       " [229, 489, 728, 229, 229],\n",
       " [229, 549, 728, 229, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 915, 728, 926, 1140],\n",
       " [229, 489, 728, 757, 229],\n",
       " [229, 131, 728, 934, 553, 229],\n",
       " [229, 131, 728, 114, 229],\n",
       " [229, 131, 728, 1269, 35, 389, 229, 271],\n",
       " [229, 489, 728, 229, 229, 131],\n",
       " [229, 489, 728, 926, 229, 107],\n",
       " [229, 131, 728, 1281, 229, 747, 958, 59],\n",
       " [229, 489, 728, 520],\n",
       " [229, 489, 728, 934, 468, 229],\n",
       " [229, 489, 728, 229, 229],\n",
       " [229, 131, 728, 926, 1140, 973],\n",
       " [229, 131, 728, 1281, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 1085, 973, 160, 1284, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 70, 728, 677, 229, 229],\n",
       " [229, 1225, 728, 1281, 973, 673],\n",
       " [229, 489, 728, 114, 229],\n",
       " [229, 489, 728, 728, 229, 229, 229],\n",
       " [229, 131, 1240, 229, 131, 43, 229],\n",
       " [229, 131, 728, 355, 229],\n",
       " [229, 131, 728, 604],\n",
       " [229, 489, 728, 926, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 604],\n",
       " [229, 549, 728, 229, 229],\n",
       " [229, 489, 728, 438, 229, 229],\n",
       " [229, 489, 728, 1085, 1036, 389, 229],\n",
       " [229, 489, 1240, 466, 43, 604],\n",
       " [229, 131, 728, 926, 229],\n",
       " [229, 549, 728, 1281, 229],\n",
       " [229, 489, 728, 1109, 229, 1284, 229],\n",
       " [229, 489, 728, 229],\n",
       " [229, 131, 728, 171, 229, 1284],\n",
       " [229, 131, 728, 677, 389, 1258],\n",
       " [229, 131, 728, 229, 229],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 489, 728, 229, 229],\n",
       " [229, 549, 728, 1267, 229],\n",
       " [229, 489, 728, 229, 229],\n",
       " [229, 131, 728, 1085, 59, 389, 229, 917],\n",
       " [229, 489, 728, 934, 229, 229],\n",
       " [229, 489, 728, 527, 229],\n",
       " [229, 549, 728, 229, 229],\n",
       " [229, 549, 728, 229, 229, 229],\n",
       " [229, 131, 728, 604],\n",
       " [229, 489, 728, 926, 229],\n",
       " [229, 131, 728, 229, 229],\n",
       " [229, 131, 728, 229, 229, 1019],\n",
       " [229, 489, 728, 229, 1138, 389],\n",
       " [229, 131, 728, 229],\n",
       " [229, 489, 728, 517, 468],\n",
       " [229, 646, 958, 466, 43, 604],\n",
       " [229, 776, 728, 517],\n",
       " [229, 489, 728, 114, 229],\n",
       " [229, 131, 728, 1281, 229],\n",
       " [229, 489, 728, 229, 229],\n",
       " [229, 489, 728, 926, 229],\n",
       " [229, 728, 728, 229],\n",
       " [229, 776, 728, 926, 1140, 229],\n",
       " [229, 489, 728, 926, 1140],\n",
       " [229, 549, 728, 1281, 229],\n",
       " [229, 915, 728, 926, 229],\n",
       " [229, 820, 728, 691, 1258, 1258],\n",
       " [229, 489, 728, 229, 229, 229],\n",
       " [229, 131, 728, 1109, 229, 917, 917],\n",
       " [229, 776, 728, 926, 229, 229],\n",
       " [229, 489, 728, 1168, 229, 229],\n",
       " [229, 1228, 685, 728, 229, 960],\n",
       " [229, 1038, 728, 926, 1140, 229],\n",
       " [229, 728, 728, 926, 1140],\n",
       " [229, 131, 728, 1085, 35, 389, 229]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [[word_dict['idx_to_word'][_id] for _id in row] for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'man', 'is', 'a', 'a', 'out', 'of', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a'],\n",
       " ['a', 'baby', 'is', 'playing', 'with'],\n",
       " ['a', 'man', 'and', 'a', 'to', 'a'],\n",
       " ['a', 'dog', 'is', 'eating', 'a'],\n",
       " ['a', 'baby', 'is', 'playing', 'a'],\n",
       " ['a', 'panda', 'is', 'playing', 'with'],\n",
       " ['a', 'man', 'is', 'riding', 'a'],\n",
       " ['a', 'woman', 'is', 'putting', 'a', 'into', 'a'],\n",
       " ['a', 'woman', 'is', 'a', 'a'],\n",
       " ['a', 'boy', 'is', 'riding', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a'],\n",
       " ['a', 'person', 'is', 'cutting', 'a'],\n",
       " ['a', 'group', 'of', 'people', 'are', 'are'],\n",
       " ['a', 'man', 'is', 'riding', 'a'],\n",
       " ['a', 'dog', 'is', 'playing', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'group', 'of', 'people', 'are'],\n",
       " ['a', 'man', 'is', 'a'],\n",
       " ['a', 'man', 'is', 'draining', 'a', 'a', 'a', 'plate'],\n",
       " ['a', 'man', 'is', 'a', 'a'],\n",
       " ['a', 'person', 'is', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'baby', 'is', 'playing', 'with'],\n",
       " ['a', 'man', 'is', 'eating', 'a'],\n",
       " ['a', 'woman', 'is', 'talking', 'to', 'a'],\n",
       " ['a', 'woman', 'is', 'peeling', 'a'],\n",
       " ['a', 'woman', 'is', 'boiling', 'eggs', 'in', 'a', 'bowl'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'woman'],\n",
       " ['a', 'man', 'is', 'playing', 'a', 'guitar'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a', 'piece', 'of', 'meat'],\n",
       " ['a', 'man', 'is', 'singing'],\n",
       " ['a', 'man', 'is', 'talking', 'on', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'playing', 'with', 'an'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'adding', 'an', 'egg', 'into', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'dog', 'is', 'riding', 'a', 'a'],\n",
       " ['a', 'chef', 'is', 'slicing', 'an', 'onion'],\n",
       " ['a', 'man', 'is', 'peeling', 'a'],\n",
       " ['a', 'man', 'is', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'woman', 'and', 'a', 'woman', 'are', 'a'],\n",
       " ['a', 'woman', 'is', 'chopping', 'a'],\n",
       " ['a', 'woman', 'is', 'dancing'],\n",
       " ['a', 'man', 'is', 'playing', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'dancing'],\n",
       " ['a', 'person', 'is', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'flying', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'adding', 'spices', 'in', 'a'],\n",
       " ['a', 'man', 'and', 'people', 'are', 'dancing'],\n",
       " ['a', 'woman', 'is', 'playing', 'a'],\n",
       " ['a', 'person', 'is', 'slicing', 'a'],\n",
       " ['a', 'man', 'is', 'stirring', 'a', 'into', 'a'],\n",
       " ['a', 'man', 'is', 'a'],\n",
       " ['a', 'woman', 'is', 'putting', 'a', 'into'],\n",
       " ['a', 'woman', 'is', 'riding', 'in', 'the'],\n",
       " ['a', 'woman', 'is', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a'],\n",
       " ['a', 'person', 'is', 'cooking', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'adding', 'meat', 'in', 'a', 'pan'],\n",
       " ['a', 'man', 'is', 'talking', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'lifting', 'a'],\n",
       " ['a', 'person', 'is', 'a', 'a'],\n",
       " ['a', 'person', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'dancing'],\n",
       " ['a', 'man', 'is', 'playing', 'a'],\n",
       " ['a', 'woman', 'is', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'a', 'a', 'finger'],\n",
       " ['a', 'man', 'is', 'a', 'food', 'in'],\n",
       " ['a', 'woman', 'is', 'a'],\n",
       " ['a', 'man', 'is', 'walking', 'on'],\n",
       " ['a', 'group', 'of', 'people', 'are', 'dancing'],\n",
       " ['a', 'cat', 'is', 'walking'],\n",
       " ['a', 'man', 'is', 'peeling', 'a'],\n",
       " ['a', 'woman', 'is', 'slicing', 'a'],\n",
       " ['a', 'man', 'is', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'a'],\n",
       " ['a', 'is', 'is', 'a'],\n",
       " ['a', 'cat', 'is', 'playing', 'with', 'a'],\n",
       " ['a', 'man', 'is', 'playing', 'with'],\n",
       " ['a', 'person', 'is', 'slicing', 'a'],\n",
       " ['a', 'baby', 'is', 'playing', 'a'],\n",
       " ['a', 'car', 'is', 'driving', 'the', 'the'],\n",
       " ['a', 'man', 'is', 'a', 'a', 'a'],\n",
       " ['a', 'woman', 'is', 'stirring', 'a', 'pan', 'pan'],\n",
       " ['a', 'cat', 'is', 'playing', 'a', 'a'],\n",
       " ['a', 'man', 'is', 'jumping', 'a', 'a'],\n",
       " ['a', 'soccer', 'player', 'is', 'a', 'basketball'],\n",
       " ['a', 'boy', 'is', 'playing', 'with', 'a'],\n",
       " ['a', 'is', 'is', 'playing', 'with'],\n",
       " ['a', 'woman', 'is', 'adding', 'eggs', 'in', 'a']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output_schedule_inference.txt', 'w') as f:\n",
    "    for i, t in zip(test_id.id, text):\n",
    "        f.write('{},{}\\n'.format(i, ' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
